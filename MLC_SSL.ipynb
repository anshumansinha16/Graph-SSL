{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9nyllOiU-Vl",
        "outputId": "d89bd29d-b2ec-44a9-b2df-88340eb71107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m‚ú®üç∞‚ú® Everything looks OK!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mamba install -c conda-forge rdkit"
      ],
      "metadata": {
        "id": "HNGqGhyWU-_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import rdkit\n",
        "from rdkit import Chem\n",
        "\n",
        "import numpy as np \n",
        "import os\n",
        "from rdkit.Chem import rdmolops\n",
        "from scipy import sparse\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from rdkit.Chem.AllChem import EmbedMolecule\n",
        "from rdkit.Chem import MolFromSmiles\n",
        "\n",
        "from rdkit.Chem import AllChem as Chem\n",
        "from rdkit.Chem import rdMolTransforms"
      ],
      "metadata": {
        "id": "ad_53P9lVA0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OgraU419486"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "!pip install git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lN4cjMd-G0t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcwhsNu2APh3"
      },
      "source": [
        "**Edge_perturbation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWr3RybY-vBA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch_geometric.data import Batch, Data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch_geometric.utils import to_dense_adj, dense_to_sparse, subgraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjvf_QcnAZYm"
      },
      "source": [
        "Part of the code is adapted from https://github.com/divelab/DIG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dbej47QSAWaT"
      },
      "outputs": [],
      "source": [
        "class EdgePerturbation():\n",
        "    \n",
        "    \"\"\"\n",
        "    Edge perturbation on the given graph or batched graphs. Class objects callable via \n",
        "    method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        add (bool, optional): Set :obj:`True` if randomly add edges in a given graph.\n",
        "            (default: :obj:`True`)\n",
        "        drop (bool, optional): Set :obj:`True` if randomly drop edges in a given graph.\n",
        "            (default: :obj:`False`)\n",
        "        ratio (float, optional): Percentage of edges to add or drop. (default: :obj:`0.1`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, add=True, drop=False, ratio=0.1):\n",
        "        self.add = add\n",
        "        self.drop = drop\n",
        "        self.ratio = ratio\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        return self.views_fn(data)\n",
        "        \n",
        "    def do_trans(self, data):\n",
        "        node_num, _ = data.x.size()\n",
        "        _, edge_num = data.edge_index.size()\n",
        "        perturb_num = int(edge_num * self.ratio)\n",
        "\n",
        "        edge_index = data.edge_index.detach().clone()\n",
        "        idx_remain = edge_index\n",
        "        idx_add = torch.tensor([]).reshape(2, -1).long()\n",
        "\n",
        "        if self.drop:\n",
        "            idx_remain = edge_index[:, np.random.choice(edge_num, edge_num-perturb_num, replace=False)]\n",
        "\n",
        "        if self.add:\n",
        "            idx_add = torch.randint(node_num, (2, perturb_num))\n",
        "\n",
        "        new_edge_index = torch.cat((idx_remain, idx_add), dim=1)\n",
        "        new_edge_index = torch.unique(new_edge_index, dim=1)\n",
        "\n",
        "        return Data(x=data.x, edge_index=new_edge_index)\n",
        "\n",
        "    def views_fn(self, data):\n",
        "        \"\"\"\n",
        "        Method to be called when :class:`EdgePerturbation` object is called.\n",
        "        \n",
        "        Args:\n",
        "            data (:class:`torch_geometric.data.Data`): The input graph or batched graphs.\n",
        "            \n",
        "        :rtype: :class:`torch_geometric.data.Data`.  \n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(data, Batch):\n",
        "            dlist = [self.do_trans(d) for d in data.to_data_list()]\n",
        "            return Batch.from_data_list(dlist)\n",
        "        elif isinstance(data, Data):\n",
        "            return self.do_trans(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX9ioEQzKMDf"
      },
      "outputs": [],
      "source": [
        "class UniformSample():\n",
        "    \"\"\"\n",
        "    Uniformly node dropping on the given graph or batched graphs. \n",
        "    Class objects callable via method :meth:`views_fn`.\n",
        "    \n",
        "    Args:\n",
        "        ratio (float, optinal): Ratio of nodes to be dropped. (default: :obj:`0.1`)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ratio=0.1):\n",
        "        self.ratio = ratio\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        return self.views_fn(data)\n",
        "    \n",
        "    def do_trans(self, data):\n",
        "        \n",
        "        node_num, _ = data.x.size()\n",
        "        _, edge_num = data.edge_index.size()\n",
        "        \n",
        "        keep_num = int(node_num * (1-self.ratio))\n",
        "        idx_nondrop = torch.randperm(node_num)[:keep_num]\n",
        "        mask_nondrop = torch.zeros_like(data.x[:,0]).scatter_(0, idx_nondrop, 1.0).bool()\n",
        "        \n",
        "        edge_index, _ = subgraph(mask_nondrop, data.edge_index, relabel_nodes=True, num_nodes=node_num)\n",
        "        return Data(x=data.x[mask_nondrop], edge_index=edge_index)\n",
        "    \n",
        "    def views_fn(self, data):\n",
        "        \"\"\"\n",
        "        Method to be called when :class:`UniformSample` object is called.\n",
        "        \n",
        "        Args:\n",
        "            data (:class:`torch_geometric.data.Data`): The input graph or batched graphs.\n",
        "            \n",
        "        :rtype: :class:`torch_geometric.data.Data`.  \n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(data, Batch):\n",
        "            dlist = [self.do_trans(d) for d in data.to_data_list()]\n",
        "            return Batch.from_data_list(dlist)\n",
        "        elif isinstance(data, Data):\n",
        "            return self.do_trans(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cekqP5eUAdwh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.utils import degree\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.data import Data, Batch, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBgGizSCAfKZ"
      },
      "outputs": [],
      "source": [
        "DATA_SPLIT = [0.7, 0.2, 0.1] # Train / val / test split ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8vMKY5iCn8Y"
      },
      "outputs": [],
      "source": [
        "def get_max_deg(dataset):\n",
        "\t\"\"\"\n",
        "\tFind the max degree across all nodes in all graphs.\n",
        "\t\"\"\"\n",
        "\tmax_deg = 0\n",
        "\trow, col = dataset[0].edge_index\n",
        "\n",
        "\tfor data in dataset:\n",
        "\t\t#print(data.edge_index.shape)\n",
        "\t\trow, col = data.edge_index\n",
        "\t\tnum_nodes = data.num_nodes\n",
        "\t\tdeg = degree(row, num_nodes)\n",
        "\t\tdeg = max(deg).item()\n",
        "\t\tif deg > max_deg:\n",
        "\t\t\tmax_deg = int(deg)\n",
        "\treturn max_deg\n",
        "\n",
        "class CatDegOnehot(object):\n",
        "\t\"\"\"\n",
        "\tAdds the node degree as one hot encodings to the node features.\n",
        "\tArgs:\n",
        "\t\tmax_degree (int): Maximum degree.\n",
        "\t\tin_degree (bool, optional): If set to :obj:`True`, will compute the in-\n",
        "\t\t\tdegree of nodes instead of the out-degree. (default: :obj:`False`)\n",
        "\t\tcat (bool, optional): Concat node degrees to node features instead\n",
        "\t\t\tof replacing them. (default: :obj:`True`)\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, max_degree, in_degree=False, cat=True):\n",
        "\t\tself.max_degree = max_degree\n",
        "\t\tself.in_degree = in_degree\n",
        "\t\tself.cat = cat\n",
        "\n",
        "\tdef __call__(self, data):\n",
        "\t\tidx, x = data.edge_index[1 if self.in_degree else 0], data.x\n",
        "\t\tif isinstance(x, torch.Tensor):\n",
        "\t\t\tpass\n",
        "\t\telse:\n",
        "\t\t\tx = torch.tensor(x)\n",
        "\n",
        "\t\t#print(x)\n",
        "\t\tdeg = degree(idx, data.num_nodes, dtype=torch.long)\n",
        "\t\tdeg = F.one_hot(deg, num_classes=self.max_degree + 1).to(torch.float)\n",
        "\n",
        "\t\tif x is not None and self.cat:\n",
        "\t\t\tx = x.view(-1, 1) if x.dim() == 1 else x\n",
        "\t\t\tdata.x = torch.cat([x, deg.to(x.dtype)], dim=-1)\n",
        "\t\telse:\n",
        "\t\t\tdata.x = deg\n",
        "\t\treturn data\n",
        "\n",
        "\n",
        "def load_dataset(name, expand_features=True):\n",
        "\t\"\"\"\n",
        "\tLoad a specific TUDataset and optionally expand the set of\n",
        "\tnode features by adding node degrees as one hot encodings.\n",
        "\tArgs:\n",
        "\t\tname (str): name of TUDataset to load\n",
        "\t\texpand_features (bool, optional): If set to :obj:`True`, will augment\n",
        "\t\t\tthe node features using their degrees. (default: :obj:`True`)\n",
        "\t\"\"\"\n",
        "\n",
        "\tif name == \"proteins\":\n",
        "\t\tdataset = TUDataset(root=\"/tmp/TUDataset/PROTEINS\", name=\"PROTEINS\", use_node_attr=True)\n",
        "\telif name == \"enzymes\":\n",
        "\t\tdataset = TUDataset(root=\"/tmp/TUDataset/ENZYMES\", name=\"ENZYMES\", use_node_attr=True)\n",
        "\telif name == \"zinc_f\":\n",
        "\t\tdataset = TUDataset(root=\"/tmp/TUDataset/ZINC_full\", name=\"ZINC_full\", use_node_attr=True)\n",
        "\telif name == \"zinc_t\":\n",
        "\t\tdataset = TUDataset(root=\"/tmp/TUDataset/ZINC_full\", name=\"ZINC_test\", use_node_attr=True)\n",
        "\telif name == \"clin_AB\":\n",
        "\t\tdataset = dataset_ab\n",
        "\telif name == \"clin_BAB\":\n",
        "\t\tdataset = dataset_bab\n",
        " \n",
        "\t#num_classes = dataset.num_classes\n",
        "\tif dataset[0].x is None or expand_features:\n",
        "\t\tmax_degree = get_max_deg(dataset)\n",
        "\t\ttransform = CatDegOnehot(max_degree)\n",
        "\t\tdataset = [transform(graph) for graph in dataset]\n",
        "\telse:\n",
        "\t\tdataset = [graph for graph in dataset]\n",
        "\tfeat_dim = dataset[0].num_node_features\n",
        "\n",
        "\treturn dataset, feat_dim#, num_classes\n",
        "\n",
        "\n",
        "def split_dataset(dataset, train_data_percent=1.0):\n",
        "\t\"\"\"\n",
        "\tSplits the data into train / val / test sets.\n",
        "\tArgs:\n",
        "\t\tdataset (list): all graphs in the dataset.\n",
        "\t\ttrain_data_percent (float): Fraction of training data\n",
        "\t\t\twhich is labelled. (default 1.0)\n",
        "\t\"\"\"\n",
        "\n",
        "\trandom.shuffle(dataset)\n",
        "\n",
        "\tn = len(dataset)\n",
        "\ttrain_split, val_split, test_split = DATA_SPLIT\n",
        "\n",
        "\ttrain_end = int(n * DATA_SPLIT[0])\n",
        "\tval_end = train_end + int(n * DATA_SPLIT[1])\n",
        "\ttrain_label_percent = int(train_end * train_data_percent)\n",
        "\ttrain_dataset, val_dataset, test_dataset = [i for i in dataset[:train_label_percent]], [i for i in dataset[train_end:val_end]], [i for i in dataset[val_end:]]\n",
        "\treturn train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def build_loader(args, dataset, subset):\n",
        "\tshuffle = (subset != \"test\")\n",
        "\tloader = DataLoader(MyDataset(dataset, subset, args.augment_list),\n",
        "\t\t\t\t\t\tnum_workers=args.num_workers, batch_size=args.batch_size, \n",
        "\t\t\t\t\t\tshuffle=shuffle, follow_batch=[\"x_anchor\", \"x_pos\"])\n",
        "\treturn loader\n",
        "\n",
        "\n",
        "def build_classification_loader(args, dataset, subset):\n",
        "\tshuffle = (subset != \"test\")\n",
        "\tloader = DataLoader(dataset, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=shuffle)\n",
        "\treturn loader\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\t\"\"\"\n",
        "\tDataset class that returns a graph and its augmented view in get() call.\n",
        "\tAugmentations are applied sequentially based on the augment_list.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, dataset, subset, augment_list):\n",
        "\t\tsuper(MyDataset, self).__init__()\n",
        "\n",
        "\t\tself.dataset = dataset\n",
        "\t\tself.augment_list = augment_list\n",
        "\n",
        "\t\tself.augment_functions = []\n",
        "\t\tfor augment in self.augment_list:\n",
        "\t\t\tif augment == \"edge_perturbation\":\n",
        "\t\t\t\tfunction = EdgePerturbation()\n",
        "\t\t\tself.augment_functions.append(function)\n",
        "\n",
        "\t\tprint(\"# samples in {} subset: {}\".format(subset, len(self.dataset)))\n",
        "\n",
        "\tdef get_positive_sample(self, current_graph):\n",
        "\t\t\"\"\"\n",
        "\t\tPossible augmentations include the following:\n",
        "\t\t\tedge_perturbation()\n",
        "\t\t\tdiffusion()\n",
        "\t\t\tdiffusion_with_sample()\n",
        "\t\t\tnode_dropping()\n",
        "\t\t\trandom_walk_subgraph()\n",
        "\t\t\tnode_attr_mask()\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tgraph_temp = current_graph\n",
        "\t\tfor function in self.augment_functions:\n",
        "\t\t\tgraph_temp = function.views_fn(graph_temp)\n",
        "\t\treturn graph_temp\n",
        "\n",
        "\tdef get(self, idx):\n",
        "\t\tgraph_anchor = self.dataset[idx]\n",
        "\t\tgraph_pos = self.get_positive_sample(graph_anchor)\n",
        "\t\treturn PairData(graph_anchor.edge_index, graph_anchor.x, graph_pos.edge_index, graph_pos.x)\n",
        "\n",
        "\tdef len(self):\n",
        "\t\treturn len(self.dataset)\n",
        "\n",
        "\n",
        "class PairData(Data):\n",
        "\t\"\"\"\n",
        "\tUtility function to return a pair of graphs in dataloader.\n",
        "\tAdapted from https://pytorch-geometric.readthedocs.io/en/latest/notes/batching.html\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, edge_index_anchor = None, x_anchor = None, edge_index_pos = None, x_pos = None):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.edge_index_anchor = edge_index_anchor\n",
        "\t\tself.x_anchor = x_anchor\n",
        "\t\t\n",
        "\t\tself.edge_index_pos = edge_index_pos\n",
        "\t\tself.x_pos = x_pos\n",
        "\n",
        "\tdef __inc__(self, key, value, *args, **kwargs):\n",
        "\t\tif key == \"edge_index_anchor\":\n",
        "\t\t\treturn self.x_anchor.size(0)\n",
        "\t\tif key == \"edge_index_pos\":\n",
        "\t\t\treturn self.x_pos.size(0)\n",
        "\t\telse:\n",
        "\t\t\treturn super().__inc__(key, value, *args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset_bab:\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "DLkh4yy90YNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBIHrrmrC2VN"
      },
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxhTUimMC20x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "import torch.nn.functional as F\n",
        "from torch_scatter import scatter_add\n",
        "from torch_geometric.nn.conv import MessagePassing\n",
        "from torch_geometric.nn.inits import glorot, zeros\n",
        "from torch.nn import Parameter, Sequential, Linear, BatchNorm1d\n",
        "from torch_geometric.utils import remove_self_loops, add_self_loops\n",
        "from torch_geometric.nn import GCNConv, GINConv, GATConv, SAGEConv, SGConv, global_add_pool, global_mean_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4C_icLUC3u5"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tA wrapper class for easier instantiation of pre-implemented graph encoders.\n",
        "\tPart of the code has been adapted from https://github.com/divelab/DIG.\n",
        "\t\n",
        "\tArgs:\n",
        "\t\tfeat_dim (int): The dimension of input node features.\n",
        "\t\thidden_dim (int): The dimension of node-level (local) embeddings. \n",
        "\t\tn_layers (int, optional): The number of GNN layers in the encoder. (default: :obj:`5`)\n",
        "\t\tpool (string, optional): The global pooling methods, :obj:`sum` or :obj:`mean`.\n",
        "\t\t\t(default: :obj:`sum`)\n",
        "\t\tgnn (string, optional): The type of GNN layer, :obj:`gcn` or :obj:`gin` or :obj:`gat`\n",
        "\t\t\tor :obj:`graphsage` or :obj:`resgcn` or :obj:`sgc`. (default: :obj:`gcn`)\n",
        "\t\tbn (bool, optional): Whether to include batch normalization. (default: :obj:`True`)\n",
        "\t\tnode_level (bool, optional): If :obj:`True`, the encoder will output node level\n",
        "\t\t\tembedding (local representations). (default: :obj:`False`)\n",
        "\t\tgraph_level (bool, optional): If :obj:`True`, the encoder will output graph level\n",
        "\t\t\tembeddings (global representations). (default: :obj:`True`)\n",
        "\t\tedge_weight (bool, optional): Only applied to GCN. Whether to use edge weight to\n",
        "\t\t\tcompute the aggregation. (default: :obj:`False`)\n",
        "\t\t\t\n",
        "\tNote\n",
        "\t----\n",
        "\tFor GCN and GIN encoders, the dimension of the output node-level (local) embedding will be \n",
        "\t:obj:`hidden_dim`, whereas the node-level embedding will be :obj:`hidden_dim` * :obj:`n_layers`. \n",
        "\tFor ResGCN, the output embeddings for boths node and graphs will have dimensions :obj:`hidden_dim`.\n",
        "\t\t\t\n",
        "\tExamples\n",
        "\t--------\n",
        "\t>>> feat_dim = dataset[0].x.shape[1]\n",
        "\t>>> encoder = Encoder(feat_dim, 128, n_layers=3, gnn=\"gin\")\n",
        "\t>>> encoder(some_batched_data).shape # graph-level embedding of shape [batch_size, 128*3]\n",
        "\t\n",
        "\t>>> encoder = Encoder(feat_dim, 128, n_layers=5, node_level=True, graph_level=False)\n",
        "\t>>> encoder(some_batched_data).shape # node-level embedding of shape [n_nodes, 128]\n",
        "\t\n",
        "\t>>> encoder = Encoder(feat_dim, 128, n_layers=5, node_level=True, graph_level=True)\n",
        "\t>>> encoder(some_batched_data) # a tuple of graph-level and node-level embeddings\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, feat_dim, hidden_dim, n_layers=5, pool=\"sum\", \n",
        "\t\t\t\t gnn=\"gcn\", bn=True, node_level=False, graph_level=True):\n",
        "\t\tsuper(Encoder, self).__init__()\n",
        "\n",
        "\t\tif gnn == \"gcn\":\n",
        "\t\t\tself.encoder = GCN(feat_dim, hidden_dim, n_layers, pool, bn)\n",
        "\t\telif gnn == \"gin\":\n",
        "\t\t\tself.encoder = GIN(feat_dim, hidden_dim, n_layers, pool, bn)\n",
        "\t\telif gnn == \"resgcn\":\n",
        "\t\t\tself.encoder = ResGCN(feat_dim, hidden_dim, n_layers, pool)\n",
        "\t\telif gnn == \"gat\":\n",
        "\t\t\tself.encoder = GAT(feat_dim, hidden_dim, n_layers, pool, bn)\n",
        "\t\telif gnn == \"graphsage\":\n",
        "\t\t\tself.encoder = GraphSAGE(feat_dim, hidden_dim, n_layers, pool, bn)\n",
        "\t\telif gnn == \"sgc\":\n",
        "\t\t\tself.encoder = SGC(feat_dim, hidden_dim, n_layers, pool, bn)\n",
        "\n",
        "\t\tself.node_level = node_level\n",
        "\t\tself.graph_level = graph_level\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\t\t\tz_g, z_n = self.encoder(data)\n",
        "\t\t\tif self.node_level and self.graph_level:\n",
        "\t\t\t\treturn z_g, z_n\n",
        "\t\t\telif self.graph_level:\n",
        "\t\t\t\treturn z_g\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn z_n\n",
        "\n",
        "\tdef save_checkpoint(self, save_path, optimizer, epoch, best_train_loss, best_val_loss, is_best):\n",
        "\t\tckpt = {}\n",
        "\t\tckpt[\"state\"] = self.state_dict()\n",
        "\t\tckpt[\"epoch\"] = epoch\n",
        "\t\tckpt[\"optimizer_state\"] = optimizer.state_dict()\n",
        "\t\tckpt[\"best_train_loss\"] = best_train_loss\n",
        "\t\tckpt[\"best_val_loss\"] = best_val_loss\n",
        "\t\ttorch.save(ckpt, os.path.join(save_path, \"model.ckpt\"))\n",
        "\t\tif is_best:\n",
        "\t\t\ttorch.save(ckpt, os.path.join(save_path, \"best_model.ckpt\"))\n",
        "\n",
        "\tdef load_checkpoint(self, load_path, optimizer):\n",
        "\t\tckpt = torch.load(os.path.join(load_path, \"best_model.ckpt\"))\n",
        "\t\tself.load_state_dict(ckpt[\"state\"])\n",
        "\t\tepoch = ckpt[\"epoch\"]\n",
        "\t\tbest_train_loss = ckpt[\"best_train_loss\"]\n",
        "\t\tbest_val_loss = ckpt[\"best_val_loss\"]\n",
        "\t\toptimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\t\treturn epoch, best_train_loss, best_val_loss\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "\t\"\"\"\n",
        "\tGraph Convolutional Network from the paper `Semi-supervised Classification\n",
        "\twith Graph Convolutional Networks <https://arxiv.org/abs/1609.02907>`.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, feat_dim, hidden_dim, n_layers=3, pool=\"sum\", bn=False, xavier=True):\n",
        "\t\tsuper(GCN, self).__init__()\n",
        "\n",
        "\t\tif bn:\n",
        "\t\t\tself.bns = torch.nn.ModuleList()\n",
        "\t\tself.convs = torch.nn.ModuleList()\n",
        "\t\tself.acts = torch.nn.ModuleList()\n",
        "\t\tself.n_layers = n_layers\n",
        "\t\tself.pool = pool\n",
        "\n",
        "\t\ta = torch.nn.ReLU()\n",
        "\n",
        "\t\tfor i in range(n_layers):\n",
        "\t\t\tstart_dim = hidden_dim if i else feat_dim\n",
        "\t\t\tconv = GCNConv(start_dim, hidden_dim)\n",
        "\t\t\tif xavier:\n",
        "\t\t\t\tself.weights_init(conv)\n",
        "\t\t\tself.convs.append(conv)\n",
        "\t\t\tself.acts.append(a)\n",
        "\t\t\tif bn:\n",
        "\t\t\t\tself.bns.append(BatchNorm1d(hidden_dim))\n",
        "\n",
        "\tdef weights_init(self, module):\n",
        "\t\tfor m in module.modules():\n",
        "\t\t\tif isinstance(m, GCNConv):\n",
        "\t\t\t\tlayer = m.lin\n",
        "\t\t\tif isinstance(m, Linear):\n",
        "\t\t\t\tlayer = m\n",
        "\t\t\ttorch.nn.init.xavier_uniform_(layer.weight.data)\n",
        "\t\t\tif layer.bias is not None:\n",
        "\t\t\t\tlayer.bias.data.fill_(0.0)\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\t\tx, edge_index, batch = data\n",
        "\t\txs = []\n",
        "\t\tfor i in range(self.n_layers):\n",
        "\t\t\tx = self.convs[i](x, edge_index)\n",
        "\t\t\tx = self.acts[i](x)\n",
        "\t\t\tif self.bns is not None:\n",
        "\t\t\t\tx = self.bns[i](x)\n",
        "\t\t\txs.append(x)\n",
        "\n",
        "\t\tif self.pool == \"sum\":\n",
        "\t\t\txpool = [global_add_pool(x, batch) for x in xs]\n",
        "\t\telse:\n",
        "\t\t\txpool = [global_mean_pool(x, batch) for x in xs]\n",
        "\t\tglobal_rep = torch.cat(xpool, 1)\n",
        "\n",
        "\t\treturn global_rep, x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VbS1ukDDyQX"
      },
      "source": [
        "**Encoder + Linear layer** : For downstream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXbdMxuZC9vb"
      },
      "outputs": [],
      "source": [
        "class GraphClassificationModel(nn.Module):\n",
        "\t\"\"\"\n",
        "\tModel for graph classification.\n",
        "\tGNN Encoder followed by linear layer.\n",
        "\t\n",
        "\tArgs:\n",
        "\t\tfeat_dim (int): The dimension of input node features.\n",
        "\t\thidden_dim (int): The dimension of node-level (local) embeddings. \n",
        "\t\tn_layers (int, optional): The number of GNN layers in the encoder. (default: :obj:`5`)\n",
        "\t\tgnn (string, optional): The type of GNN layer, :obj:`gcn` or :obj:`gin` or :obj:`gat`\n",
        "\t\t\tor :obj:`graphsage` or :obj:`resgcn` or :obj:`sgc`. (default: :obj:`gcn`)\n",
        "\t\tload (string, optional): The SSL model to be loaded. The GNN encoder will be\n",
        "\t\t\tinitialized with pretrained SSL weights, and only the classifier head will\n",
        "\t\t\tbe trained. Otherwise, GNN encoder and classifier head are trained end-to-end.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self, feat_dim, hidden_dim, n_layers, output_dim, gnn, load=None):\n",
        "\t\tsuper(GraphClassificationModel, self).__init__()\n",
        "\n",
        "\t\tself.encoder = Encoder(feat_dim, hidden_dim, n_layers=n_layers, gnn=gnn)\n",
        "\n",
        "\t\tif load:\n",
        "\t\t\tckpt = torch.load(os.path.join(\"logs\", load, \"best_model.ckpt\"))\n",
        "\t\t\tself.encoder.load_state_dict(ckpt[\"state\"])\n",
        "\t\t\tfor param in self.encoder.parameters():\n",
        "\t\t\t\tparam.requires_grad = False\n",
        "\n",
        "\t\tif gnn in [\"resgcn\", \"sgc\"]:\n",
        "\t\t\tfeat_dim = hidden_dim\n",
        "\t\telse:\n",
        "\t\t\tfeat_dim = n_layers * hidden_dim\n",
        "\t\tself.classifier = nn.Linear(feat_dim, output_dim)\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\t\tembeddings = self.encoder(data)\n",
        "\t\tscores = self.classifier(embeddings)\n",
        "\t\treturn scores\n",
        "\n",
        "\tdef save_checkpoint(self, save_path, optimizer, epoch, best_train_loss, best_val_loss, is_best):\n",
        "\t\tckpt = {}\n",
        "\t\tckpt[\"state\"] = self.state_dict()\n",
        "\t\tckpt[\"epoch\"] = epoch\n",
        "\t\tckpt[\"optimizer_state\"] = optimizer.state_dict()\n",
        "\t\tckpt[\"best_train_loss\"] = best_train_loss\n",
        "\t\tckpt[\"best_val_loss\"] = best_val_loss\n",
        "\t\ttorch.save(ckpt, os.path.join(save_path, \"pred_model.ckpt\"))\n",
        "\t\tif is_best:\n",
        "\t\t\ttorch.save(ckpt, os.path.join(save_path, \"best_pred_model.ckpt\"))\n",
        "\n",
        "\tdef load_checkpoint(self, load_path, optimizer):\n",
        "\t\tckpt = torch.load(os.path.join(load_path, \"best_pred_model.ckpt\"))\n",
        "\t\tself.load_state_dict(ckpt[\"state\"])\n",
        "\t\tepoch = ckpt[\"epoch\"]\n",
        "\t\tbest_train_loss = ckpt[\"best_train_loss\"]\n",
        "\t\tbest_val_loss = ckpt[\"best_val_loss\"]\n",
        "\t\toptimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
        "\t\treturn epoch, best_train_loss, best_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlZXjD_ID-58"
      },
      "source": [
        "**Loss function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qImjKpa4D7I1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z87x8BfD9x_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class infonce(nn.Module):\n",
        "\t\"\"\"\n",
        "\tThe InfoNCE (NT-XENT) loss in contrastive learning. The implementation\n",
        "\tfollows the paper `A Simple Framework for Contrastive Learning of \n",
        "\tVisual Representations <https://arxiv.org/abs/2002.05709>`.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(infonce, self).__init__()\n",
        "\n",
        "\t\tself.tau = 0.5\n",
        "\t\tself.norm = True\n",
        "\n",
        "\tdef forward(self, embed_anchor, embed_positive):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tembed_anchor, embed_positive: Tensor of shape [batch_size, embed_dim]\n",
        "\t\t\ttau: Float. Usually in (0,1].\n",
        "\t\t\tnorm: Boolean. Whether to apply normlization.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tbatch_size = embed_anchor.shape[0]\n",
        "\t\tsim_matrix = torch.einsum(\"ik,jk->ij\", embed_anchor, embed_positive)\n",
        "\n",
        "\t\tif self.norm:\n",
        "\t\t\tembed_anchor_abs = embed_anchor.norm(dim=1)\n",
        "\t\t\tembed_positive_abs = embed_positive.norm(dim=1)\n",
        "\t\t\tsim_matrix = sim_matrix / torch.einsum(\"i,j->ij\", embed_anchor_abs, embed_positive_abs)\n",
        "\n",
        "\t\tsim_matrix = torch.exp(sim_matrix / self.tau)\n",
        "\t\tpos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "\t\tloss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
        "\t\tloss = - torch.log(loss).mean()\n",
        "\t\treturn loss\n",
        "\n",
        "\n",
        "class jensen_shannon(nn.Module):\n",
        "\t\"\"\"\n",
        "\tThe Jensen-Shannon Estimator of Mutual Information used in contrastive learning. The\n",
        "\timplementation follows the paper `Learning deep representations by mutual information \n",
        "\testimation and maximization <https://arxiv.org/abs/1808.06670>`.\n",
        "\n",
        "\tNote: The JSE loss implementation can produce negative values because a :obj:`-2log2` shift is \n",
        "\t\tadded to the computation of JSE, for the sake of consistency with other f-convergence losses.\n",
        "\t\"\"\"\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(jensen_shannon, self).__init__()\n",
        "\n",
        "\tdef get_expectation(self, masked_d_prime, positive=True):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tmasked_d_prime: Tensor of shape [n_graphs, n_graphs] for global_global,\n",
        "\t\t\t\t\t\t\ttensor of shape [n_nodes, n_graphs] for local_global.\n",
        "\t\t\tpositive (bool): Set True if the d_prime is masked for positive pairs,\n",
        "\t\t\t\t\t\t\tset False for negative pairs.\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tlog_2 = np.log(2.)\n",
        "\t\tif positive:\n",
        "\t\t\tscore = log_2 - F.softplus(-masked_d_prime)\n",
        "\t\telse:\n",
        "\t\t\tscore = F.softplus(-masked_d_prime) + masked_d_prime - log_2\n",
        "\t\treturn score\n",
        "\n",
        "\tdef forward(self, embed_anchor, embed_positive):\n",
        "\t\t\"\"\"\n",
        "\t\tArgs:\n",
        "\t\t\tembed_anchor, embed_positive: Tensor of shape [batch_size, embed_dim].\n",
        "\t\t\"\"\"\n",
        "\n",
        "\t\tdevice = embed_anchor.device\n",
        "\t\tbatch_size = embed_anchor.shape[0]\n",
        "\n",
        "\t\tpos_mask = torch.zeros((batch_size, batch_size)).to(device)\n",
        "\t\tneg_mask = torch.ones((batch_size, batch_size)).to(device)\n",
        "\t\tfor graphidx in range(batch_size):\n",
        "\t\t\tpos_mask[graphidx][graphidx] = 1.\n",
        "\t\t\tneg_mask[graphidx][graphidx] = 0.\n",
        "\n",
        "\t\td_prime = torch.matmul(embed_anchor, embed_positive.t())\n",
        "\n",
        "\t\tE_pos = self.get_expectation(d_prime * pos_mask, positive=True).sum()\n",
        "\t\tE_pos = E_pos / batch_size\n",
        "\t\tE_neg = self.get_expectation(d_prime * neg_mask, positive=False).sum()\n",
        "\t\tE_neg = E_neg / (batch_size * (batch_size - 1))\n",
        "\t\treturn E_neg - E_pos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlZs_UhoEF1J"
      },
      "source": [
        "Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajgjryg8GlNL",
        "outputId": "cefb1a7c-8e4d-438c-fd7c-8f15d0b544cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/site-packages (2.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/site-packages (from tensorboardX) (3.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from tensorboardX) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from tensorboardX) (1.24.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6qvFEIEEGNh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "from tensorboardX import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTCGVymRMxVN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn8dfHyiEJeb"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_seed(seed):\n",
        "\t\"\"\"\n",
        "\tUtility function to set seed values for RNG for various modules\n",
        "\t\"\"\"\n",
        "\tnp.random.seed(seed)\n",
        "\ttorch.manual_seed(seed)\n",
        "\ttorch.cuda.manual_seed(seed)\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class Options:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.parser = argparse.ArgumentParser(description=\"Self-Supervised Learning for Graphs\")\n",
        "\t\tself.parser.add_argument(\"--save\", dest=\"save\", action=\"store\", required=True)\n",
        "\t\tself.parser.add_argument(\"--lr\", dest=\"lr\", action=\"store\", default=0.001, type=float)\n",
        "\t\tself.parser.add_argument(\"--epochs\", dest=\"epochs\", action=\"store\", default=20, type=int)\n",
        "\t\tself.parser.add_argument(\"--batch_size\", dest=\"batch_size\", action=\"store\", default=64, type=int)\n",
        "\t\tself.parser.add_argument(\"--num_workers\", dest=\"num_workers\", action=\"store\", default=8, type=int)\n",
        "\t\tself.parser.add_argument(\"--dataset\", dest=\"dataset\", action=\"store\", required=True, type=str,\n",
        "\t\t\tchoices=[\"proteins\", \"enzymes\", \"collab\" ,\"reddit_binary\", \"reddit_multi\", \"imdb_binary\", \"imdb_multi\", \"dd\", \"mutag\", \"nci1\"])\n",
        "\t\tself.parser.add_argument(\"--model\", dest=\"model\", action=\"store\", default=\"gcn\", type=str,\n",
        "\t\t\tchoices=[\"gcn\", \"gin\", \"resgcn\", \"gat\", \"graphsage\", \"sgc\"])\n",
        "\t\tself.parser.add_argument(\"--feat_dim\", dest=\"feat_dim\", action=\"store\", default=128, type=int)\n",
        "\t\tself.parser.add_argument(\"--layers\", dest=\"layers\", action=\"store\", default=3, type=int)\n",
        "\t\tself.parser.add_argument(\"--loss\", dest=\"loss\", action=\"store\", default=\"infonce\", type=str, choices=[\"infonce\", \"jensen_shannon\"])\n",
        "\t\tself.parser.add_argument(\"--augment_list\", dest=\"augment_list\", nargs=\"*\", default=[\"edge_perturbation\", \"node_dropping\"], type=str,\n",
        "\t\t\tchoices=[\"edge_perturbation\", \"diffusion\", \"diffusion_with_sample\", \"node_dropping\", \"random_walk_subgraph\", \"node_attr_mask\"])\n",
        "\t\tself.parser.add_argument(\"--train_data_percent\", dest=\"train_data_percent\", action=\"store\", default=1.0, type=float)\n",
        "\n",
        "\t\tself.parse()\n",
        "\t\tself.check_args()\n",
        "\n",
        "\tdef parse(self):\n",
        "\t\tself.opts = self.parser.parse_args()\n",
        "\n",
        "\tdef check_args(self):\n",
        "\t\tif not os.path.isdir(os.path.join(\"runs\", self.opts.save)):\n",
        "\t\t\tos.makedirs(os.path.join(\"runs\", self.opts.save))\n",
        "\t\tif not os.path.isdir(os.path.join(\"logs\", self.opts.save)):\n",
        "\t\t\tos.makedirs(os.path.join(\"logs\", self.opts.save))\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn (\"All Options:\\n\" + \"\".join([\"-\"] * 45) + \"\\n\" + \"\\n\".join([\"{:<18} -------> {}\".format(k, v) for k, v in vars(self.opts).items()]) + \"\\n\" + \"\".join([\"-\"] * 45) + \"\\n\")\n",
        "\n",
        "\n",
        "def run(args, epoch, mode, dataloader, model, optimizer):\n",
        "\t\n",
        "\t\tif mode == \"train\":\n",
        "\t\t\tmodel.train()\n",
        "\t\telif mode == \"val\" or mode == \"test\":\n",
        "\t\t\tmodel.eval()\n",
        "\t\telse:\n",
        "\t\t\tassert False, \"Wrong Mode:{} for Run\".format(mode)\n",
        "\n",
        "\t\tlosses = []\n",
        "\t\tcontrastive_fn = eval(args.loss + \"()\")\n",
        "\t\twith trange(len(dataloader), desc=\"{}, Epoch {}: \".format(mode, epoch)) as t:\n",
        "\t\t\tfor data in dataloader:\n",
        "\t\t\t\tdata.to(device)\n",
        "\n",
        "\t\t\t\t# readout_anchor is the embedding of the original datapoint x on passing through the model\n",
        "\t\t\t\treadout_anchor = model((data.x_anchor, data.edge_index_anchor, data.x_anchor_batch))\n",
        "\n",
        "\t\t\t\t# readout_positive is the embedding of the positively augmented x on passing through the model\n",
        "\t\t\t\treadout_positive = model((data.x_pos, data.edge_index_pos, data.x_pos_batch))\n",
        "\n",
        "\t\t\t\t# negative samples for calculating the contrastive loss is computed in contrastive_fn\n",
        "\t\t\t\tloss = contrastive_fn(readout_anchor, readout_positive)\n",
        "\n",
        "\t\t\t\tif mode == \"train\":\n",
        "\t\t\t\t\t# backprop\n",
        "\t\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\t\tloss.backward()\n",
        "\t\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t\t# keep track of loss values\n",
        "\t\t\t\tlosses.append(loss.item())\n",
        "\t\t\t\tt.set_postfix(loss=losses[-1])\n",
        "\t\t\t\tt.update()\n",
        "\n",
        "\t\t# gather the results for the epoch\n",
        "\t\tepoch_loss = sum(losses) / len(losses)\n",
        "\t\treturn epoch_loss\n",
        "\n",
        "def main(args):\n",
        "\tdataset, input_dim, num_classes = load_dataset(args.dataset)\n",
        " \n",
        "\t# split the data into train / val / test sets\n",
        "\ttrain_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "\t# build_loader is a dataloader which gives a paired sampled - the original x and the positively \n",
        "\t# augmented x obtained by applying the transformations in the augment_list as an argument\n",
        "\ttrain_loader = build_loader(args, train_dataset, \"train\")\n",
        "\tval_loader = build_loader(args, val_dataset, \"val\")\n",
        "\ttest_loader = build_loader(args, test_dataset, \"test\")\n",
        "\n",
        "\t# easy initialization of the GNN model encoder to map graphs to embeddings needed for contrastive training\n",
        "\tmodel = Encoder(input_dim, args.feat_dim, n_layers=args.layers, gnn=args.model)\n",
        "\tmodel = model.to(device)\n",
        "\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\tbest_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "\tlogger = SummaryWriter(logdir = os.path.join(\"runs\", args.save))\n",
        "\n",
        "\tfor epoch in range(args.epochs):\n",
        "\t\ttrain_loss = run(args, epoch, \"train\", train_loader, model, optimizer)\n",
        "\t\tprint(\"Train Epoch Loss: {}\".format(train_loss))\n",
        "\t\tlogger.add_scalar(\"Train Loss\", train_loss, epoch)\n",
        "\n",
        "\t\tval_loss = run(args, epoch, \"val\", val_loader, model, optimizer)\n",
        "\t\tprint(\"Val Epoch Loss: {}\".format(val_loss))\n",
        "\t\tlogger.add_scalar(\"Val Loss\", val_loss, epoch)\n",
        "\n",
        "\t\t# save model\n",
        "\t\tis_best_loss = False\n",
        "\t\tif val_loss < best_val_loss:\n",
        "\t\t\tbest_epoch, best_train_loss, best_val_loss, is_best_loss = epoch, train_loss, val_loss, True\n",
        "\n",
        "\t\tmodel.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "\tprint(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "\tprint(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))\n",
        "\n",
        "\tbest_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttest_loss = run(args, best_epoch, \"test\", test_loader, model, optimizer)\n",
        "\tprint(\"Test Loss at epoch {}: {:.3f}\".format(best_epoch, test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnZOwtbgR7kt"
      },
      "outputs": [],
      "source": [
        "args.device "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jVBpDERHBfi"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tset_seed(0)\n",
        "\targs = Options()\n",
        "\tprint(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gr1axvwIEgFk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "Change these arguments to change either the dataset / model / loss function / types of augmentations.\n",
        "The augmentations mentioned in augment_list shall be applied sequentially to generate a positive pair for contrastive training.\n",
        "Make sure to not add too many augmentations as that would change the fundamental structure of the input graph.\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"save\" : \"ssl_model\",\n",
        "    \"lr\" : 0.001,\n",
        "    \"epochs\" : 20,\n",
        "    \"batch_size\" : 32,\n",
        "    \"num_workers\" : 2,\n",
        "    \"dataset\" : \"zinc_f\", # Choices are [\"proteins\", \"enzymes\", \"zinc_f\"]\n",
        "    \"model\" : \"gcn\", # choices are [\"gcn\"]\n",
        "    \"feat_dim\" : 64,\n",
        "    \"layers\" : 3,\n",
        "    \"loss\" : \"infonce\", # choices are [\"infonce\", \"jensen_shannon\"]\n",
        "    \"augment_list\" : [\"edge_perturbation\", \"node_dropping\"],\n",
        "    \"train_data_percent\" : 1.0,\n",
        "}\n",
        "\n",
        "class AttributeDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttributeDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "args = AttributeDict(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming that the 'dataset.pkl' file is in the current directory\n",
        "with open('datasetBAB.pkl', 'rb') as f:\n",
        "    dataset_bab = pickle.load(f)"
      ],
      "metadata": {
        "id": "CBmFyvmwVqrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Assuming that the 'dataset.pkl' file is in the current directory\n",
        "with open('datasetAB.pkl', 'rb') as f:\n",
        "    dataset_ab = pickle.load(f)"
      ],
      "metadata": {
        "id": "uB7hPNe4ZTYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip my_folder.zip -d /content/"
      ],
      "metadata": {
        "id": "JqOzE2AZaEvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_ab[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZp4tlsOYx-m",
        "outputId": "74becb58-7376-4d02-abd2-fefb41b76e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[24, 5], edge_index=[2, 46], edge_attr=[23, 3], y=[1], smiles='*C(=O)[C@H](CCCCNC(=O)OCCOC)NC(=O)OCCOC')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset_ab:\n",
        "  pass"
      ],
      "metadata": {
        "id": "I7jEeQ1nmBzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Dataset, Data\n",
        "import numpy as np \n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
        "print(f\"Torch geometric version: {torch_geometric.__version__}\")\n",
        "\n",
        "\"\"\"\n",
        "!!!\n",
        "NOTE: This file was replaced by dataset_featurizer.py\n",
        "but is kept to illustrate how to build a custom dataset in PyG.\n",
        "!!!\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class MoleculeDataset(Dataset):\n",
        "    def __init__(self, root, filename, test=False, transform=None, pre_transform=None):\n",
        "        \"\"\"\n",
        "        root = Where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
        "        \"\"\"\n",
        "        self.test = test\n",
        "        self.filename = filename\n",
        "        super(MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
        "            (The download func. is not implemented here)  \n",
        "        \"\"\"\n",
        "        return self.filename\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
        "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "        if self.test:\n",
        "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
        "        else:\n",
        "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        self.data = pd.read_csv(self.raw_paths[0])\n",
        "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
        "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
        "            # Get node features\n",
        "            node_feats = self._get_node_features(mol_obj)\n",
        "            # Get edge features\n",
        "            edge_feats = self._get_edge_features(mol_obj)\n",
        "            # Get adjacency info\n",
        "            edge_index = self._get_adjacency_info(mol_obj)\n",
        "            # Get labels info\n",
        "            label = self._get_labels(mol[\"HIV_active\"])\n",
        "\n",
        "            # Create data object\n",
        "            data = Data(x=node_feats, \n",
        "                        edge_index=edge_index,\n",
        "                        edge_attr=edge_feats,\n",
        "                        y=label,\n",
        "                        smiles=mol[\"smiles\"]\n",
        "                        ) \n",
        "            if self.test:\n",
        "                torch.save(data, \n",
        "                    os.path.join(self.processed_dir, \n",
        "                                 f'data_test_{index}.pt'))\n",
        "            else:\n",
        "                torch.save(data, \n",
        "                    os.path.join(self.processed_dir, \n",
        "                                 f'data_{index}.pt'))\n",
        "\n",
        "    def _get_node_features(self, mol):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of Nodes, Node Feature size]\n",
        "        \"\"\"\n",
        "        all_node_feats = []\n",
        "\n",
        "        for atom in mol.GetAtoms():\n",
        "            node_feats = []\n",
        "            # Feature 1: Atomic number        \n",
        "            node_feats.append(atom.GetAtomicNum())\n",
        "            # Feature 2: Atom degree\n",
        "            node_feats.append(atom.GetDegree())\n",
        "            # Feature 3: Formal charge\n",
        "            node_feats.append(atom.GetFormalCharge())\n",
        "            # Feature 4: Hybridization\n",
        "            node_feats.append(atom.GetHybridization())\n",
        "            # Feature 5: Aromaticity\n",
        "            node_feats.append(atom.GetIsAromatic())\n",
        "            # Feature 6: Total Num Hs\n",
        "            node_feats.append(atom.GetTotalNumHs())\n",
        "            # Feature 7: Radical Electrons\n",
        "            node_feats.append(atom.GetNumRadicalElectrons())\n",
        "            # Feature 8: In Ring\n",
        "            node_feats.append(atom.IsInRing())\n",
        "            # Feature 9: Chirality\n",
        "            node_feats.append(atom.GetChiralTag())\n",
        "\n",
        "            # Append node features to matrix\n",
        "            all_node_feats.append(node_feats)\n",
        "\n",
        "        all_node_feats = np.asarray(all_node_feats)\n",
        "        return torch.tensor(all_node_feats, dtype=torch.float)\n",
        "\n",
        "    def _get_edge_features(self, mol):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of edges, Edge Feature size]\n",
        "        \"\"\"\n",
        "        all_edge_feats = []\n",
        "\n",
        "        for bond in mol.GetBonds():\n",
        "            edge_feats = []\n",
        "            # Feature 1: Bond type (as double)\n",
        "            edge_feats.append(bond.GetBondTypeAsDouble())\n",
        "            # Feature 2: Rings\n",
        "            edge_feats.append(bond.IsInRing())\n",
        "            # Append node features to matrix (twice, per direction)\n",
        "            all_edge_feats += [edge_feats, edge_feats]\n",
        "\n",
        "        all_edge_feats = np.asarray(all_edge_feats)\n",
        "        return torch.tensor(all_edge_feats, dtype=torch.float)\n",
        "\n",
        "    def _get_adjacency_info(self, mol):\n",
        "        \"\"\"\n",
        "        We could also use rdmolops.GetAdjacencyMatrix(mol)\n",
        "        but we want to be sure that the order of the indices\n",
        "        matches the order of the edge features\n",
        "        \"\"\"\n",
        "        edge_indices = []\n",
        "        for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            edge_indices += [[i, j], [j, i]]\n",
        "\n",
        "        edge_indices = torch.tensor(edge_indices)\n",
        "        edge_indices = edge_indices.t().to(torch.long).view(2, -1)\n",
        "        return edge_indices\n",
        "\n",
        "    def _get_labels(self, label):\n",
        "        label = np.asarray([label])\n",
        "        return torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    def len(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def get(self, idx):\n",
        "        \"\"\" - Equivalent to __getitem__ in pytorch\n",
        "            - Is not needed for PyG's InMemoryDataset\n",
        "        \"\"\"\n",
        "        #if self.test:\n",
        "        #    data = torch.load(os.path.join(self.processed_dir, \n",
        "        #                         f'data_test_{idx}.pt'))\n",
        "        #else:\n",
        "        data = torch.load(os.path.join(self.processed_dir, \n",
        "                                  f'data_{idx}.pt'))   \n",
        "        return data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pczDeXf2U2FZ",
        "outputId": "4e70208e-4b9a-4fdf-e68d-1daa127e74c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.0.0+cu118\n",
            "Cuda available: False\n",
            "Torch geometric version: 2.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Graph_MoleculeDataset(Dataset):\n",
        "    def __init__(self, filename, root, test=False, transform=None, pre_transform=None):\n",
        "        \"\"\"\n",
        "        root = Where the dataset should be stored. This folder is split\n",
        "        into raw_dir (downloaded dataset) and processed_dir (processed data). \n",
        "        \"\"\"\n",
        "        #self.test = test\n",
        "        self.filename = filename\n",
        "        super(Graph_MoleculeDataset, self).__init__(root, transform, pre_transform)\n",
        "        \n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        \"\"\" If this file exists in raw_dir, the download is not triggered.\n",
        "            (The download func. is not implemented here)  \n",
        "        \"\"\"\n",
        "        return self.filename\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        \"\"\" If these files are found in raw_dir, processing is skipped\"\"\"\n",
        "        '''\n",
        "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "\n",
        "        if self.test:\n",
        "            return [f'data_test_{i}.pt' for i in list(self.data.index)]\n",
        "        else:\n",
        "            return [f'data_{i}.pt' for i in list(self.data.index)]\n",
        "        '''\n",
        "        return 'not_implemented'\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        self.data = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "        #featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
        "        for index, mol in tqdm(self.data.iterrows(), total=self.data.shape[0]):\n",
        "            # Featurize molecule\n",
        "            #mol3d = Chem.MolFromSmiles(smiles)\n",
        "            mol_obj = Chem.MolFromSmiles(mol[\"smiles\"])\n",
        "            if mol_obj:\n",
        "              node_feats = self._get_node_features(mol_obj)\n",
        "              edge_feats = self._get_edge_features(mol_obj)\n",
        "              edge_index = self._get_adjacency_info(mol_obj)\n",
        "              label = self._get_labels(mol[\"CT_TOX\"])\n",
        "\n",
        "            #print(edge_feats)\n",
        "\n",
        "            data = Data(x=node_feats, \n",
        "                        edge_index = edge_index,\n",
        "                        edge_attr = edge_feats,\n",
        "                        y = label,\n",
        "                        smiles = mol[\"smiles\"]\n",
        "                        )\n",
        "            torch.save(data,\n",
        "                       os.path.join(self.processed_dir,\n",
        "                                    f'data_{index}.pt'))\n",
        "\n",
        "    def _get_node_features(self, mol):\n",
        "\n",
        "        all_node_feats = []\n",
        "\n",
        "        try:\n",
        "\n",
        "          #mol3d = Chem.AddHs(mol)\n",
        "          #mol3d = Chem.RemoveHs(mol3d)\n",
        "          mol3d = mol\n",
        "          Chem.EmbedMolecule(mol3d, randomSeed=42)\n",
        "          Chem.MMFFOptimizeMolecule(mol3d)\n",
        "    \n",
        "          conf = mol3d.GetConformer() # atom1 = bond.GetBeginAtom()\n",
        "\n",
        "          for bond in mol3d.GetBonds():\n",
        "\n",
        "              node_feats = []\n",
        "              atom1 = bond.GetBeginAtom()\n",
        "              atom2 = bond.GetEndAtom()\n",
        "\n",
        "              # Feature 1: Atomic number        \n",
        "              node_feats.append(atom1.GetAtomicNum())\n",
        "              node_feats.append(atom2.GetAtomicNum())\n",
        "              \n",
        "              bl = rdMolTransforms.GetBondLength(mol3d.GetConformer(), atom1.GetIdx(), atom2.GetIdx())\n",
        "              b_lengths = round(bl,2)\n",
        "\n",
        "              # Feature 1: Bond type (as double)\n",
        "              node_feats.append(b_lengths)\n",
        "              # Feature 2: Bond type (as double)\n",
        "              node_feats.append(float(bond.GetBondTypeAsDouble()))\n",
        "              # Feature 3: Rings\n",
        "              if bond.IsInRing():\n",
        "                node_feats.append(1.0)\n",
        "              else:\n",
        "                node_feats.append(0.0)\n",
        "\n",
        "              # Append node features to matrix\n",
        "              all_node_feats.append(node_feats)\n",
        "\n",
        "          return np.asarray(all_node_feats)\n",
        "\n",
        "        except:\n",
        "          return None\n",
        "\n",
        "    def _get_edge_features(self, mol):\n",
        "        \"\"\" \n",
        "        This will return a matrix / 2d array of the shape\n",
        "        [Number of edges, Edge Feature size]\n",
        "        \"\"\"\n",
        "        all_edge_feats = []\n",
        "\n",
        "        try:\n",
        "\n",
        "          #mol3d = Chem.AddHs(mol)\n",
        "          #mol3d = Chem.RemoveHs(mol3d)\n",
        "          #mol = MolFromSmiles('CCO')\n",
        "\n",
        "          mol3d = mol\n",
        "          Chem.EmbedMolecule(mol3d, randomSeed=42)\n",
        "          Chem.MMFFOptimizeMolecule(mol3d)\n",
        "          \n",
        "          conf = mol3d.GetConformer() # atom1 = bond.GetBeginAtom()\n",
        "\n",
        "          b_lengths = []\n",
        "\n",
        "          # Compute the bond angles of all non-hydrogen bonds\n",
        "          for bond in mol3d.GetBonds():\n",
        "\n",
        "              edge_feats = []\n",
        "\n",
        "              # Skip hydrogen bonds\n",
        "              if bond.GetBeginAtom().GetAtomicNum() == 1 or bond.GetEndAtom().GetAtomicNum() == 1:\n",
        "                  continue\n",
        "              \n",
        "              # Get the three atoms involved in the bond\n",
        "              atom1 = bond.GetBeginAtom()\n",
        "              atom2 = bond.GetEndAtom()\n",
        "              atom3 = None\n",
        "\n",
        "              for atom in atom1.GetNeighbors():\n",
        "                  if atom.GetIdx() != atom2.GetIdx() and atom.GetAtomicNum() != 1:\n",
        "                      atom3 = atom\n",
        "                      bl = rdMolTransforms.GetBondLength(conf, atom1.GetIdx(), atom3.GetIdx())\n",
        "                      flag1 = 1\n",
        "                      break\n",
        "\n",
        "              for atom in atom2.GetNeighbors():\n",
        "                  if atom.GetIdx() != atom1.GetIdx() and atom.GetAtomicNum() != 1:\n",
        "                      atom3 = atom\n",
        "                      bl = rdMolTransforms.GetBondLength(conf, atom2.GetIdx(), atom3.GetIdx())\n",
        "                      flag2 = 1\n",
        "                      break\n",
        "\n",
        "              angle = rdMolTransforms.GetAngleDeg(conf, atom1.GetIdx(), atom2.GetIdx(), atom3.GetIdx())\n",
        "              edge_feats.append(angle)\n",
        "              #di_angle = rdMolTransforms.GetDihedralDeg(conf, atom1.GetIdx(), atom2.GetIdx(), atom3.GetIdx())\n",
        "              #edge_feats.append(di_angle)\n",
        "\n",
        "              all_edge_feats.append(edge_feats)\n",
        "              #print('edge_feat : ', all_edge_feats)\n",
        "\n",
        "          all_edge_feats = np.asarray(all_edge_feats)\n",
        "          return torch.tensor(all_edge_feats, dtype = torch.float)\n",
        "\n",
        "        except:\n",
        "          return None\n",
        "\n",
        "\n",
        "    def _get_adjacency_info(self, mol):\n",
        "      \"\"\" \n",
        "      This will return a matrix / 2d array of the shape\n",
        "      [Number of edges, Edge Feature size]\n",
        "      \"\"\"\n",
        "      try:\n",
        "\n",
        "        mol3d = Chem.AddHs(mol)\n",
        "        mol3d = Chem.RemoveHs(mol3d)\n",
        "        Chem.EmbedMolecule(mol3d, randomSeed=42)\n",
        "        Chem.MMFFOptimizeMolecule(mol3d)\n",
        "      \n",
        "        conf = mol3d.GetConformer()\n",
        "\n",
        "        b_lengths = []\n",
        "        ba_graph = []\n",
        "        conf = mol.GetConformer()\n",
        "\n",
        "        # Compute the bond angles of all non-hydrogen bonds\n",
        "        for bond in mol.GetBonds():\n",
        "\n",
        "            atom1 = bond.GetBeginAtom()\n",
        "            atom2 = bond.GetEndAtom()\n",
        "\n",
        "            if bond.GetBeginAtom().GetAtomicNum() == 1 or bond.GetEndAtom().GetAtomicNum() == 1:\n",
        "                continue\n",
        "            \n",
        "            # Get the three atoms involved in the bond\n",
        "            atom1 = bond.GetBeginAtom()\n",
        "            atom2 = bond.GetEndAtom()\n",
        "            atom3 = None\n",
        "\n",
        "            ba1 = (atom1.GetIdx(), atom2.GetIdx())\n",
        "            \n",
        "\n",
        "            for atom in atom1.GetNeighbors():\n",
        "                if atom.GetIdx() != atom2.GetIdx() and atom.GetAtomicNum() != 1:\n",
        "                    atom3 = atom\n",
        "                    bl = rdMolTransforms.GetBondLength(conf, atom1.GetIdx(), atom3.GetIdx())\n",
        "                    flag1 = 1\n",
        "                    #break\n",
        "\n",
        "                    ba2 = (atom1.GetIdx(), atom3.GetIdx())\n",
        "                    ba_edge = (ba1, ba2)\n",
        "                    #print('ba_edge-',ba_edge)\n",
        "                    ba_graph.append(ba_edge)\n",
        "            \n",
        "            for atom in atom2.GetNeighbors():\n",
        "                if atom.GetIdx() != atom1.GetIdx() and atom.GetAtomicNum() != 1:\n",
        "                    atom3 = atom\n",
        "                    bl = rdMolTransforms.GetBondLength(conf, atom2.GetIdx(), atom3.GetIdx())\n",
        "                    flag2 = 1\n",
        "                    #break\n",
        "\n",
        "                    ba2 = (atom2.GetIdx(), atom3.GetIdx())\n",
        "                    ba_edge = (ba1, ba2)\n",
        "                    #print('ba_edge*',ba_edge)\n",
        "                    ba_graph.append(ba_edge)\n",
        "\n",
        "\n",
        "            dic = {}\n",
        "            count = 0\n",
        "            edge_in = []\n",
        "\n",
        "            for i in ba_graph:\n",
        "              \n",
        "              if tuple(sorted(i[0])) in dic:\n",
        "                first_ind = dic[tuple(sorted(i[0]))]\n",
        "              else:\n",
        "                dic[tuple(sorted(i[0]))] = count\n",
        "                first_ind = count\n",
        "                count+=1\n",
        "\n",
        "              \n",
        "              if tuple(sorted(i[1])) in dic:\n",
        "                second_ind = dic[tuple(sorted(i[1]))]\n",
        "              else:\n",
        "                dic[tuple(sorted(i[1]))] = count\n",
        "                second_ind = count\n",
        "                count+=1\n",
        "\n",
        "              edge_i = [first_ind, second_ind]\n",
        "              edge_in.append(edge_i)\n",
        "\n",
        "        return torch.tensor(edge_in, dtype = torch.long)\n",
        "      \n",
        "      except:\n",
        "        return None\n",
        "\n",
        "    def _get_labels(self, label):\n",
        "        label = np.asarray([label])\n",
        "        return torch.tensor(label, dtype=torch.int64)\n",
        "\n",
        "    def len(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def get(self, idx):\n",
        "        \"\"\" Equivalent to __getitem__ in pytorch \"\"\"\n",
        "        data = torch.load(os.path.join(self.processed_dir, \n",
        "                                 f'data_{idx}.pt'))\n",
        "        return data"
      ],
      "metadata": {
        "id": "GTHBhS2vYt_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7WDIP-rSKzAH",
        "outputId": "cae853cf-d948-4469-888b-40d4bf46ba66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'zinc_t'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor([3.1382])\n",
        "torch.Size([2, 26])\n",
        "tensor([ 0,  1,  1,  1,  4,  4,  5,  6,  7,  7,  8,  9, 10, 10, 11, 12, 12, 14,\n",
        "        15, 15, 16, 17, 18, 19, 20, 22])\n",
        "tensor([ 1,  2,  3,  4,  5, 23,  6,  7,  8, 22,  9, 10, 11, 22, 12, 13, 14, 15,\n",
        "        16, 20, 17, 18, 19, 20, 21, 23])"
      ],
      "metadata": {
        "id": "5GSMJWrJek7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDcTvF_CElfk"
      },
      "outputs": [],
      "source": [
        "dataset, input_dim = load_dataset(args.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset_ab"
      ],
      "metadata": {
        "id": "vyJL0WglYPwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYn_rOc_ZYkI",
        "outputId": "536a9233-7ab5-4c2f-a52e-59d5700de61d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 36], x=[33, 33], edge_attr=[36, 3], y=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s--GDNVhy-W",
        "outputId": "c158436d-4f51-4815-bf6d-d3b750a9a127"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# samples in train subset: 174619\n",
            "# samples in val subset: 49891\n",
            "# samples in test subset: 24946\n"
          ]
        }
      ],
      "source": [
        "# split the data into train / val / test sets\n",
        "#For a given graph, its positive is constructed using the data augmentation \n",
        "#methods discussed earlier, and all other graphs in the mini-batch constitute as negatives\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "# build_loader is a dataloader which gives a paired sampled - the original x and the positively \n",
        "# augmented x obtained by applying the transformations in the augment_list as an argument\n",
        "train_loader = build_loader(args, train_dataset, \"train\")\n",
        "val_loader = build_loader(args, val_dataset, \"val\")\n",
        "test_loader = build_loader(args, test_dataset, \"test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEen2kyru8cy",
        "outputId": "9e855f01-6eb2-4f5a-e971-12cb7e954a46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# samples in test subset: 1484\n"
          ]
        }
      ],
      "source": [
        "test_loader = build_loader(args, dataset, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7oHXrNHW-Z9",
        "outputId": "67822ed3-6475-4faa-d48d-4ec33f127eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'device': device(type='cpu'),\n",
              " 'save': 'ssl_model',\n",
              " 'lr': 0.001,\n",
              " 'epochs': 20,\n",
              " 'batch_size': 32,\n",
              " 'num_workers': 2,\n",
              " 'dataset': 'zinc_f',\n",
              " 'model': 'gcn',\n",
              " 'feat_dim': 64,\n",
              " 'layers': 3,\n",
              " 'loss': 'infonce',\n",
              " 'augment_list': ['edge_perturbation', 'node_dropping'],\n",
              " 'train_data_percent': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LHr7hp4jIOS"
      },
      "outputs": [],
      "source": [
        "data.x_anchor_ptr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVSQF1FWOm6b"
      },
      "outputs": [],
      "source": [
        "for i in train_loader:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZurfkEVEnYY"
      },
      "outputs": [],
      "source": [
        "# easy initialization of the GNN model encoder to map graphs to embeddings needed for contrastive training \n",
        "model = Encoder(input_dim, args.feat_dim, n_layers=args.layers, gnn=args.model) \n",
        "model = model.to(args.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXHxJSVZUW_M",
        "outputId": "ad040f51-cd8c-40d3-e71d-43e640739493"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "HNNEwNBNEryc",
        "outputId": "8f30b370-9484-4429-a4fd-06042937688b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rtrain, Epoch 0:   0%|          | 0/5457 [00:00<?, ?it/s]Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
            "    obj = _ForkingPickler.dumps(obj)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
            "    cls(buf, protocol).dump(obj)\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
            "    obj = _ForkingPickler.dumps(obj)\n",
            "_pickle.PicklingError: Can't pickle <class '__main__.PairData'>: it's not the same object as __main__.PairData\n",
            "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
            "    cls(buf, protocol).dump(obj)\n",
            "_pickle.PicklingError: Can't pickle <class '__main__.PairData'>: it's not the same object as __main__.PairData\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
            "    obj = _ForkingPickler.dumps(obj)\n",
            "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
            "    cls(buf, protocol).dump(obj)\n",
            "_pickle.PicklingError: Can't pickle <class '__main__.PairData'>: it's not the same object as __main__.PairData\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/multiprocessing/queues.py\", line 244, in _feed\n",
            "    obj = _ForkingPickler.dumps(obj)\n",
            "  File \"/usr/lib/python3.9/multiprocessing/reduction.py\", line 51, in dumps\n",
            "    cls(buf, protocol).dump(obj)\n",
            "_pickle.PicklingError: Can't pickle <class '__main__.PairData'>: it's not the same object as __main__.PairData\n",
            "train, Epoch 0:   0%|          | 0/5457 [02:13<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-8b3173a0ecf8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# def run(args, epoch, mode, dataloader, model, optimizer):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Epoch {}, Train Loss: {:.3f}, Val Loss: {:.3f}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-debdc5c405c1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(args, epoch, mode, dataloader, model, optimizer)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mcontrastive_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"{}, Epoch {}: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m                                 \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if not os.path.isdir(os.path.join(\"logs\", args.save)):\n",
        "    os.makedirs(os.path.join(\"logs\", args.save))\n",
        "\n",
        "best_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss = run(args, epoch, \"train\", train_loader,model, optimizer) # def run(args, epoch, mode, dataloader, model, optimizer):\n",
        "    val_loss = run(args,epoch, \"val\", val_loader,model, optimizer)\n",
        "    log = \"Epoch {}, Train Loss: {:.3f}, Val Loss: {:.3f}\"\n",
        "    print(log.format(epoch, train_loss, val_loss))\n",
        "\n",
        "    # save model\n",
        "    is_best_loss = False\n",
        "    if val_loss < best_val_loss:\n",
        "        best_epoch, best_train_loss, best_val_loss, is_best_loss = \\\n",
        "                                            epoch, train_loss, val_loss, True\n",
        "\n",
        "    model.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, \n",
        "                          best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "print(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "print(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzGoHyXkEuHl",
        "outputId": "ad394c78-cef5-468a-e1b6-1d2ff8492af0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rtest, Epoch 14:   0%|          | 0/390 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = elem.storage()._new_shared(numel)\n",
            "/usr/local/lib/python3.9/dist-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  storage = elem.storage()._new_shared(numel)\n",
            "test, Epoch 14: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 390/390 [00:29<00:00, 13.41it/s, loss=2.02]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss at epoch 14: 2.281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "best_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "model.eval()\n",
        "\n",
        "test_loss = run(args, best_epoch, \"test\", test_loader, model, optimizer)\n",
        "print(\"Test Loss at epoch {}: {:.3f}\".format(best_epoch, test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6p62J1OJAGE"
      },
      "source": [
        "**Embedding generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuICNoWRI_XN"
      },
      "outputs": [],
      "source": [
        "class Embgen(nn.Module):\n",
        "\tdef __init__(self, feat_dim, hidden_dim, n_layers, gnn, load=None):\n",
        "\t\tsuper(Embgen, self).__init__()\n",
        "\t\tprint(hidden_dim)\n",
        "\t\tself.encoder = Encoder(feat_dim, hidden_dim, n_layers=n_layers, gnn=gnn)\n",
        "\n",
        "\t\tif load:\n",
        "\t\t\tckpt = torch.load(os.path.join(\"logs\", load, \"best_model.ckpt\"))\n",
        "\t\t\tself.encoder.load_state_dict(ckpt[\"state\"])\n",
        "\t\t\tfor param in self.encoder.parameters():\n",
        "\t\t\t\tparam.requires_grad = False\n",
        "\n",
        "\tdef forward(self, data):\n",
        "\t\tembeddings,emb2 = self.encoder(data)\n",
        "\t\t#scores = self.classifier(embeddings)\n",
        "\t\treturn embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "74h4jmmLLWtW",
        "outputId": "7254c9a1-a975-45e8-d0ed-64f1c88b3a4f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gcn'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "args.model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, input_dim = load_dataset(args.dataset)"
      ],
      "metadata": {
        "id": "TaSg65m2FQK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.feat_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO499RgYXfSr",
        "outputId": "5390975f-cea7-4725-d588-cad4b2fed0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9P73r1BJu3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48714b54-27f5-4ae0-ab78-87ad22bf538b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n"
          ]
        }
      ],
      "source": [
        "# dataset, input_dim, num_classes = test_dataset\n",
        "model_x = Embgen(input_dim, args.feat_dim, n_layers=args.layers, gnn=args.model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.x_anchor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnLJOPJ4oRiz",
        "outputId": "d58dcb0a-99eb-4888-b09a-556897b96c62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[34.,  0.,  0.,  4.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = build_loader(args, dataset, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K71__UV5oxaK",
        "outputId": "1f3c655b-c975-4b07-c832-8740f4906732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# samples in test subset: 1484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[210].y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmKu3aqBuyQM",
        "outputId": "ad06b21d-e8b9-42e5-bc46-adda820a0fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data in test_loader:\n",
        "    print(data)"
      ],
      "metadata": {
        "id": "sxbP1ClepxzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cowDfoMK0Fh"
      },
      "outputs": [],
      "source": [
        "data_emb = []\n",
        "rab = []\n",
        "for data in test_loader:\n",
        "      try:\n",
        "        # readout_anchor is the embedding of the original datapoint x on passing through the model\n",
        "        readout_anchor, ra = model_x((data.x_anchor, data.edge_index_anchor, data.x_anchor_batch))\n",
        "        data_emb.append(readout_anchor)\n",
        "        rab.append(ra)\n",
        "      except:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_emb"
      ],
      "metadata": {
        "id": "1s-NLu9EzxeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0IQOKSNwwmz"
      },
      "outputs": [],
      "source": [
        "with open('data_emb.pkl', 'wb') as f:\n",
        "  pickle.dump(data_emb, f)\n",
        "  f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFWQRikQvwBF"
      },
      "outputs": [],
      "source": [
        "data_act = []\n",
        "for data in dataset:\n",
        "  data.to(device)\n",
        "  data_act.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VT_w0uqMnkw"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "# open a file in write mode\n",
        "with open('emb_data.csv', 'w', newline='') as csvfile:\n",
        "    \n",
        "    # create a CSV writer object\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    # write the header row\n",
        "    #csvwriter.writerow(['First Name', 'Last Name', 'Age'])\n",
        "    # write the data rows\n",
        "    for row in data_emb:\n",
        "        csvwriter.writerow(row)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dim red"
      ],
      "metadata": {
        "id": "a9ZtdG35pkOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = pd.read_csv('/content/emb_data.csv',header = None)"
      ],
      "metadata": {
        "id": "fk1jWVjNplaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "F2_PBX6y7i3t",
        "outputId": "d6d2371a-0773-495e-c97e-b0d90a7788d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   0\n",
              "0  tensor([ 0.0000e+00, -2.9802e-07,  7.3314e-06,...\n",
              "1  tensor([ 0.0000e+00,  1.7881e-07, -7.4506e-07,...\n",
              "2  tensor([ 0.0000e+00,  1.4901e-07,  1.0133e-06,...\n",
              "3  tensor([ 0.0000e+00, -3.5763e-07, -8.5086e-06,...\n",
              "4  tensor([ 0.0000e+00, -4.4703e-07,  3.0100e-06,..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1105e02-252d-4db0-b476-4269a964106c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tensor([ 0.0000e+00, -2.9802e-07,  7.3314e-06,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tensor([ 0.0000e+00,  1.7881e-07, -7.4506e-07,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tensor([ 0.0000e+00,  1.4901e-07,  1.0133e-06,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tensor([ 0.0000e+00, -3.5763e-07, -8.5086e-06,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tensor([ 0.0000e+00, -4.4703e-07,  3.0100e-06,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1105e02-252d-4db0-b476-4269a964106c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1105e02-252d-4db0-b476-4269a964106c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1105e02-252d-4db0-b476-4269a964106c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_1 = dt.iloc[:, 0].tolist()"
      ],
      "metadata": {
        "id": "2GN2gc-q9XvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(dataset)):\n",
        "  if(dataset[i].edge_index.shape[1]==0):\n",
        "    print(i)\n",
        "    print(dataset[i].y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJxrT-yq91RW",
        "outputId": "2b0fd6d6-f200-482c-aef1-ad997ef49046"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_v = []\n",
        "for i in range(len(dataset)):\n",
        "  y_v.append(dataset[i].y)"
      ],
      "metadata": {
        "id": "ug9q0OO78iln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_1[0]"
      ],
      "metadata": {
        "id": "DQS3aORX_LQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dbms = data_emb"
      ],
      "metadata": {
        "id": "lvbyKBAaHM9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_item = y_v.pop(1400) "
      ],
      "metadata": {
        "id": "49GdPdKC-tf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out2[:,0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpT1Z_CvO5JR",
        "outputId": "b54929ef-bbf4-465c-89e5-4978e64bbb82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1483, 192)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from mpl_toolkits import mplot3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "#x_reshaped = x_test[0:500].reshape(-1, 784)              # new shape is (500, 28*28) = (500, 784)\n",
        "#out2 = torch.tensor(data_emb)\n",
        "# 10 out of 6230 output enteries all in out\n",
        "#print(out2[0].shape) #torch.Size([10, 708])\n",
        "\n",
        "y_color_map = y_v#.to(trainer.device)\n",
        "#print('color',y_color_map.shape)\n",
        "#print(y_color_map)\n",
        "out2 = torch.stack(data_emb)\n",
        "out2 = out2.detach().numpy()\n",
        "#out2 = scaler.inverse_transform(out2[0].cpu().data.numpy())\n",
        "\n",
        "x_scaled = StandardScaler().fit_transform(out2[:,0])    # center and scale data (mean=0, std=1)\n",
        "x_transformed = pca.fit(x_scaled).transform(x_scaled)\n",
        "\n",
        "plt.figure()\n",
        "cm = plt.cm.get_cmap('RdYlBu')\n",
        "sc = plt.scatter(x_transformed[:, 0], x_transformed[:, 1],\n",
        "            s=20, alpha=.8 , cmap=cm, c= y_color_map) # cmap='Set1', c=y_test[0:500]\n",
        "\n",
        "plt.title(\"PCA train data 1000 samples\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.ylim(-5,5)\n",
        "plt.xlim(0,0.10)\n",
        "plt.colorbar(sc)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "mqsDN8JdJPFr",
        "outputId": "c7b0c9e0-e1c5-4891-fc3e-d4bc917a05a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-65c87857b8a1>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#out2 = scaler.inverse_transform(out2[0].cpu().data.numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mx_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# center and scale data (mean=0, std=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mx_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 't'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_emb[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2w-DQMVNJxP",
        "outputId": "2f434cbe-db44-4c60-a434-281937da386f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 192])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out2[100].t().shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKWzOawJJ65N",
        "outputId": "e702569c-a32d-4522-de58-64db13292183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([192, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "#x_reshaped = x_test[0:500].reshape(-1, 784)              # new shape is (500, 28*28) = (500, 784)\n",
        "out2 = data_emb #model(x_pca, y_pca)\n",
        "# 10 out of 6230 output enteries all in out\n",
        "#print(out2[0].shape) #torch.Size([10, 708])\n",
        "\n",
        "y_v = torch.tensor(y_v)\n",
        "out2 = torch.tensor(out2)\n",
        "\n",
        "y_color_map = y_v#.to(trainer.device)\n",
        "#print('color',y_color_map.shape)\n",
        "#print(y_color_map)\n",
        "\n",
        "out2 = out2.detach().numpy()\n",
        "#out2 = scaler.inverse_transform(out2[0].cpu().data.numpy())\n",
        "\n",
        "\n",
        "#x_scaled = StandardScaler().fit_transform(out2)    # center and scale data (mean=0, std=1)\n",
        "#x_transformed = pca.fit(x_scaled).transform(x_scaled)\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "cm = plt.cm.get_cmap('RdYlBu')\n",
        "sc = plt.scatter(out2[:, 0], out2[:, 1],\n",
        "            s=20, alpha=.8 , cmap=cm, c= y_color_map) # cmap='Set1', c=y_test[0:500]\n",
        "\n",
        "plt.title(\"PCA train data 1000 samples\")\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(sc)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "wcKpuvSp856e",
        "outputId": "1cc33e61-0d53-432e-998c-45e539a58bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-57-d1baa353870e>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y_v = torch.tensor(y_v)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-d1baa353870e>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0my_color_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_v\u001b[0m\u001b[0;31m#.to(trainer.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxt9gpXE1Dx"
      },
      "source": [
        "**Downstream**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sbSukq0Fa3p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import trange\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_seed(seed):\n",
        "\t\"\"\"\n",
        "\tUtility function to set seed values for RNG for various modules\n",
        "\t\"\"\"\n",
        "\tnp.random.seed(seed)\n",
        "\ttorch.manual_seed(seed)\n",
        "\ttorch.cuda.manual_seed(seed)\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "\ttorch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class Options:\n",
        "\n",
        "\tdef __init__(self):\n",
        "\t\tself.parser = argparse.ArgumentParser(description=\"Classification Task on Graphs\")\n",
        "\t\tself.parser.add_argument(\"--save\", dest=\"save\", action=\"store\", required=True)\n",
        "\t\tself.parser.add_argument(\"--load\", dest=\"load\", action=\"store\")\n",
        "\t\tself.parser.add_argument(\"--lr\", dest=\"lr\", action=\"store\", default=0.001, type=float)\n",
        "\t\tself.parser.add_argument(\"--epochs\", dest=\"epochs\", action=\"store\", default=20, type=int)\n",
        "\t\tself.parser.add_argument(\"--batch_size\", dest=\"batch_size\", action=\"store\", default=64, type=int)\n",
        "\t\tself.parser.add_argument(\"--num_workers\", dest=\"num_workers\", action=\"store\", default=8, type=int)\n",
        "\t\tself.parser.add_argument(\"--dataset\", dest=\"dataset\", action=\"store\", required=True, type=str,\n",
        "\t\t\tchoices=[\"proteins\", \"enzymes\", \"collab\", \"reddit_binary\", \"reddit_multi\", \"imdb_binary\", \"imdb_multi\", \"dd\", \"mutag\", \"nci1\"])\n",
        "\t\tself.parser.add_argument(\"--model\", dest=\"model\", action=\"store\", default=\"gcn\", type=str,\n",
        "\t\t\tchoices=[\"gcn\", \"gin\", \"resgcn\", \"gat\", \"graphsage\", \"sgc\"])\n",
        "\t\tself.parser.add_argument(\"--feat_dim\", dest=\"feat_dim\", action=\"store\", default=128, type=int)\n",
        "\t\tself.parser.add_argument(\"--layers\", dest=\"layers\", action=\"store\", default=3, type=int)\n",
        "\t\tself.parser.add_argument(\"--train_data_percent\", dest=\"train_data_percent\", action=\"store\", default=1.0, type=float)\n",
        "\n",
        "\t\tself.parse()\n",
        "\t\tself.check_args()\n",
        "\n",
        "\tdef parse(self):\n",
        "\t\tself.opts = self.parser.parse_args()\n",
        "\n",
        "\tdef check_args(self):\n",
        "\t\tif not os.path.isdir(os.path.join(\"runs\", self.opts.save)):\n",
        "\t\t\tos.makedirs(os.path.join(\"runs\", self.opts.save))\n",
        "\t\tif not os.path.isdir(os.path.join(\"logs\", self.opts.save)):\n",
        "\t\t\tos.makedirs(os.path.join(\"logs\", self.opts.save))\n",
        "\n",
        "\tdef __str__(self):\n",
        "\t\treturn (\"All Options:\\n\" + \"\".join([\"-\"] * 45) + \"\\n\" + \"\\n\".join([\"{:<18} -------> {}\".format(k, v) for k, v in vars(self.opts).items()]) + \"\\n\" + \"\".join([\"-\"] * 45) + \"\\n\")\n",
        "\n",
        "\n",
        "def run(args, epoch, mode, dataloader, model, optimizer):\n",
        "\tif mode == \"train\":\n",
        "\t\tmodel.train()\n",
        "\telif mode == \"val\" or mode == \"test\":\n",
        "\t\tmodel.eval()\n",
        "\telse:\n",
        "\t\tassert False, \"Wrong Mode:{} for Run\".format(mode)\n",
        "\n",
        "\t# CrossEntropy loss since it is a classification task\n",
        "\tloss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\tlosses = []\n",
        "\tcorrect = 0\n",
        "\twith trange(len(dataloader), desc=\"{}, Epoch {}: \".format(mode, epoch)) as t:\n",
        "\t\tfor data in dataloader:\n",
        "\t\t\tdata.to(device)\n",
        "\n",
        "\t\t\tdata_input = data.x, data.edge_index, data.batch\n",
        "\t\t\tlabels = data.y\n",
        "\n",
        "\t\t\t# get class scores from model\n",
        "\t\t\tscores = model(data_input)\n",
        "\n",
        "\t\t\t# compute cross entropy loss\n",
        "\t\t\tloss = loss_fn(scores, labels)\n",
        "\n",
        "\t\t\tif mode == \"train\":\n",
        "\t\t\t\t# backprop\n",
        "\t\t\t\toptimizer.zero_grad()\n",
        "\t\t\t\tloss.backward()\n",
        "\t\t\t\toptimizer.step()\n",
        "\n",
        "\t\t\t# keep track of loss and accuracy\n",
        "\t\t\tpred = scores.argmax(dim=1)\n",
        "\t\t\tcorrect += int((pred == labels).sum())\n",
        "\t\t\tlosses.append(loss.item())\n",
        "\t\t\tt.set_postfix(loss=losses[-1])\n",
        "\t\t\tt.update()\n",
        "\n",
        "\t# gather the results for the epoch\n",
        "\tepoch_loss = sum(losses) / len(losses)\n",
        "\taccuracy = correct / len(dataloader.dataset)\n",
        "\treturn epoch_loss, accuracy\n",
        "\n",
        "\n",
        "def main(args):\n",
        "\tdataset, input_dim, num_classes = load_dataset(args.dataset)\n",
        "\n",
        "\t# split the data into train / val / test sets\n",
        "\ttrain_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "\t# build_classification_loader is a dataloader which gives one graph at a time\n",
        "\ttrain_loader = build_classification_loader(args, train_dataset, \"train\")\n",
        "\tval_loader = build_classification_loader(args, val_dataset, \"val\")\n",
        "\ttest_loader = build_classification_loader(args, test_dataset, \"test\")\n",
        "\n",
        "\tprint(\"Dataset Split: {} {} {}\".format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "\tprint(\"Number of Classes: {}\".format(num_classes))\n",
        "\n",
        "\t# classification model is a GNN encoder followed by linear layer\n",
        "\tmodel = GraphClassificationModel(input_dim, args.feat_dim, n_layers=args.layers, output_dim=num_classes, gnn=args.model, load=args.load)\n",
        "\tmodel = model.to(device)\n",
        "\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "\tbest_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "\tlogger = SummaryWriter(logdir = os.path.join(\"runs\", args.save))\n",
        "\n",
        "\tbest_valid_epoch = 0\n",
        "\tfor epoch in range(args.epochs):\n",
        "\t\ttrain_loss, train_acc = run(args, epoch, \"train\", train_loader, model, optimizer)\n",
        "\t\tprint(\"Train Epoch Loss: {}, Accuracy: {}\".format(train_loss, train_acc))\n",
        "\t\tlogger.add_scalar(\"Train Loss\", train_loss, epoch)\n",
        "\n",
        "\t\tval_loss, val_acc = run(args, epoch, \"val\", val_loader, model, optimizer)\n",
        "\t\tprint(\"Val Epoch Loss: {}, Accuracy: {}\".format(val_loss,val_acc))\n",
        "\t\tlogger.add_scalar(\"Val Loss\", val_loss, epoch)\n",
        "\n",
        "\t\t# save model\n",
        "\t\tis_best_loss = False\n",
        "\t\tif val_loss < best_val_loss:\n",
        "\t\t\tbest_epoch, best_train_loss, best_val_loss, is_best_loss = epoch, train_loss, val_loss, True\n",
        "\t\t\tbest_valid_epoch = epoch\n",
        "\n",
        "\t\tmodel.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "\tprint(\"Epoch for best validation loss :\", best_valid_epoch)\n",
        "\tprint(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "\tprint(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))\n",
        "\n",
        "\tbest_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttest_loss, test_accuracy = run(args, best_epoch, \"test\", test_loader, model, optimizer)\n",
        "\tprint(\"Test Loss at epoch {}: {:.3f}, Test Accuracy: {:.3f}\".format(best_epoch, test_loss, test_accuracy))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\tset_seed(0)\n",
        "\targs = Options()\n",
        "\tprint(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qY19yVdIE4pN"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Change these arguments to change either the dataset / model / train data percent\n",
        "train_data_percent is the fraction of training data which has labels associated. The utility of self-supervised \n",
        "training can be seen when train_data_percent is low and we can't train the entire model end-to-end.\n",
        "NOTE: The load argument will be the same as the save argument from the self-supervised training procedure\n",
        "'''\n",
        "\n",
        "args = {\n",
        "    \"device\" : torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    \"save\" : \"downstream_model\",\n",
        "    \"load\" : \"ssl_model\",\n",
        "    \"lr\" : 0.001,\n",
        "    \"epochs\" : 20,\n",
        "    \"batch_size\" : 64,\n",
        "    \"num_workers\" : 2,\n",
        "    \"dataset\" : \"proteins\", # Choices are [\"proteins\", \"enzymes\", \"collab\", \n",
        "                            # \"reddit_binary\", \"reddit_multi\", \"imdb_binary\", \n",
        "                            # \"imdb_multi\", \"dd\", \"mutag\", \"nci1\"]\n",
        "    \"model\" : \"gcn\", # choices are [\"gcn\", \"gin\", \"resgcn\", \"gat\", \"graphsage\", \"sgc\"]\n",
        "    \"feat_dim\" : 128,\n",
        "    \"layers\" : 3,\n",
        "    \"train_data_percent\" : 1.0,\n",
        "}\n",
        "\n",
        "args = AttributeDict(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIJ9GlWgE7iS"
      },
      "outputs": [],
      "source": [
        "dataset, input_dim = load_dataset(args.dataset)\n",
        "\n",
        "# split the data into train / val / test sets\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, args.train_data_percent)\n",
        "\n",
        "# build_classification_loader is a dataloader which gives one graph at a time\n",
        "train_loader = build_classification_loader(args, train_dataset, \"train\")\n",
        "val_loader = build_classification_loader(args, val_dataset, \"val\")\n",
        "test_loader = build_classification_loader(args, test_dataset, \"test\")\n",
        "\n",
        "print(\"Dataset split: {} {} {}\".format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
        "#print(\"Number of classes: {}\".format(num_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miM1SoaWE_TQ"
      },
      "outputs": [],
      "source": [
        "# classification model is a GNN encoder followed by linear layer\n",
        "model = GraphClassificationModel(input_dim, args.feat_dim, n_layers=args.layers, output_dim=num_classes, gnn=args.model, load=args.load)\n",
        "model = model.to(args.device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_09Z4tlFB8R"
      },
      "outputs": [],
      "source": [
        "if not os.path.isdir(os.path.join(\"logs\", args.save)):\n",
        "    os.makedirs(os.path.join(\"logs\", args.save))\n",
        "\n",
        "best_train_loss, best_val_loss = float(\"inf\"), float(\"inf\")\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    train_loss, train_acc = run(epoch, \"train\", train_loader)\n",
        "    val_loss, val_acc = run(epoch, \"val\", val_loader)\n",
        "    log = \"Epoch {}, Train Loss: {:.3f}, Train Accuracy: {:.3f}, Val Loss: {:.3f}, Val Accuracy: {:.3f}\"\n",
        "    print(log.format(epoch, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "    # save model\n",
        "    is_best_loss = False\n",
        "    if val_loss < best_val_loss:\n",
        "        best_epoch, best_train_loss, best_val_loss, is_best_loss = epoch, train_loss, val_loss, True\n",
        "\n",
        "    model.save_checkpoint(os.path.join(\"logs\", args.save), optimizer, epoch, best_train_loss, best_val_loss, is_best_loss)\n",
        "\n",
        "print(\"Train Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_train_loss))\n",
        "print(\"Val Loss at epoch {} (best model): {:.3f}\".format(best_epoch, best_val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gb5JxIYLFqtS"
      },
      "outputs": [],
      "source": [
        "best_epoch, best_train_loss, best_val_loss = model.load_checkpoint(os.path.join(\"logs\", args.save), optimizer)\n",
        "model.eval()\n",
        "\n",
        "test_loss, test_accuracy = run(best_epoch, \"test\", test_loader)\n",
        "print(\"Test Loss at epoch {}: {:.3f}, Test Accuracy: {:.3f}\".format(best_epoch, test_loss, test_accuracy))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}